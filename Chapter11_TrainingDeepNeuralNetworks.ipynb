{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradient Problems\n",
    "\n",
    "Gradients often get smaller as the algorithm progresses down to lower layers, leaving them virtually unchanged. This is the #vanishing gradients# problem. \n",
    "\n",
    "The opposite of this is the #exploding gradients# problem where the gradients will get larger.\n",
    "\n",
    "Deep neural networks suffer from unstable gradients generally because different layers learn at different speeds. \n",
    "\n",
    "The reason is that there logistic sigmoid function and a certain weight initialization in the early 2000's was part of the reason.\n",
    "\n",
    "Main reason is that the vairance of the output for each layer is much greater than the variance of the inputs.\n",
    "\n",
    "### Glorot and He Initialization\n",
    "\n",
    "Argued that we need the vairance of the outputs of each layer to match its inputs, and the gradients need to have equal variance before and after flowing in the reverse direction.\n",
    "\n",
    "Not possible to do guarantee both unless layer has equal number of inputs and neurons(called *fan-in* and *fan-out* of the layer).\n",
    "\n",
    "The Xavier or Glorot Initialization is fan<sub>avg</sub> = (fan<sub>in</sub> + fan<sub>out</sub>) / 2\n",
    "\n",
    "By default, Keras uses Glorot initialization with uniform distribution.\n",
    "\n",
    "The He initialization is also a name for ReLu.\n",
    "\n",
    "Below is a list of initializers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "\n",
    "ReLu is a great choice, but it suffers from *dying ReLu's*, where some neurons only output 0. Huge portions of the network can die, esp. with high learning rates.\n",
    "\n",
    "To deal with that issue, the made *leaky ReLu* where when its less than zero it becomes negative instead of 0, and it then has a chance of neurons coming back to life.\n",
    "\n",
    "High leaks seem to do really well. Parametric leaky Relu (PReLu) can be modified through backprop. Its really good for large image datasets but tends to overfit on smaller ones.\n",
    "\n",
    "Then there is *exponential linear unit* (ELU) that performs better than ReLu. Its main con is that its slower to compute than ReLu and its variants. That it converges faster during training compensates but at test time it will be slower.\n",
    "\n",
    "*Scaled ELU* (SELU) will self-normalize (if you only use dense layers, and all use SELU), preserving a mean of 0 and std of 1, which solves the gradient problems. It will outperform other activation functions, but it has conditions:\n",
    "    - Input features must be standardized(mean=0, std=1)\n",
    "    - Every hidden layer weights must be initialized with LeCun normal initialization. `kernel_initializer=\"lecun_normal\"`\n",
    "    - Network architecture must be sequential, if used on recurrent networks or networks that are Wide & Deep, self-normalization not guaranteed and performance wont necessairly outperform other functions.\n",
    "    - Only dense layers, but can improve convolutional neural nets as well.\n",
    "    \n",
    "\n",
    "The general path is SELU > ELU > leaky ReLu & variants > ReLu > tanh > logistic.\n",
    "\n",
    "If network architecure prevents self-normalizing, got to ELU. If I care about runtime latency, leaky Relu (I can set a value for it). Can also cross-validation other unctions, such as RReLu if network is overfitting or PReLu if there is a huge training set. BUT ReLu has a lot of support and is fast.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
