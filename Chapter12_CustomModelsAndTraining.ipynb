{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and Operations (ops)\n",
    "\n",
    "A tensor is very similar to an `ndarray` from NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tensor representing a matrix with 2 rows & 3 cols\n",
    "tf.constant([[1.,2.,3.], [4.,5.,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42) # representing a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1.,2.,3.], [4.,5.,6.]])\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it has a shape\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and a dtype\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing is like NumPy\n",
    "t[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not exactly sure how this one works\n",
    "t[...,1,tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: this will basically call tf.add(t,10)\n",
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All basic math operations are available. Whenever the name differs, there is a good reason. For example, the `tf.transpose` cannot be written like `t.T`. In TF, a new tensor is created with its own copy of data vs in Numpy where its just a transposed view of the data.\n",
    "\n",
    "### Tensors and NumPy\n",
    "\n",
    "I can apply tensor operations to numpy and numpy ops to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2.,4.,5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor to array\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor to array\n",
    "np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicting types\n",
    "\n",
    "**Tensorflow does not do type conversions automatically!!**\n",
    "\n",
    "Type conversions hurt performance and often done automatically. TF doesn't do them, but will raise an exception if I try to execute an operation on incompatible types.\n",
    "\n",
    "If I need to change a type, I can use `tf.cast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:Mul]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) * tf.constant(40)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print (ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40.,dtype=tf.float64)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print (ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "So far the `tf.Tensor` values have been immmutable. Wont work for backprop or modifying optimizers. So, introducing, `tf.Variable`.\n",
    "\n",
    "It will act like a tf.Tensor, perform same ops, works with NumPy, and picky with types. BUT, also can be modified in place by the `assign()` method. \n",
    "\n",
    "Can also modify individual cells by `assign()` or by using `scatter_update()` or `scatter_nd_update()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[1.,2.,3.], [4.,5.,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiplying the whole matrix\n",
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing one value in place\n",
    "v[0,1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changin the last column of the matrix\n",
    "v[:, 2].assign([0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ResourceVariable' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "#cant use just the equal sign\n",
    "try:\n",
    "    v[1] = [7.,8.,9.]\n",
    "except TypeError as ex:\n",
    "    print (ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings\n",
    "\n",
    "Regular tensors of type `tf.string`. Byte strings, not Unicode. \n",
    "\n",
    "Can be represented as `tf.int32`, the `tf.strings` package has ops for both byte stings and Unicode, and conversions. \n",
    "\n",
    "The length wont appear in a tf.string, but will in a tf.int32 where the letters are represented as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Hello world'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(b\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"café\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tf.constant([ord(c) for c in \"café\"])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
    "tf.strings.length(b, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(b, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has converted the words into lengths of strings,\n",
    "# maybe calls one of the previous functions to encode it?\n",
    "tf.strings.length(p, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tf.strings.unicode_decode(p, \"UTF8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged Tensors\n",
    "\n",
    "Static lists of lists of tensors, each tensor has teh same shape and data type. `tf.ragged` for ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 67 111 102 102 101 101], shape=(6,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# a single tensor from the variable r\n",
    "print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232]]>\n"
     ]
    }
   ],
   "source": [
    "# calling two of these tensors makes it ragged\n",
    "print(r[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857], [65, 66], [], [67]]>\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "r2 = tf.ragged.constant([[65,66], [], [67]])\n",
    "print(tf.concat([r,r2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101, 71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\n"
     ]
    }
   ],
   "source": [
    "r3 = tf.ragged.constant([[68,69,70], [71], [], [72,73]])\n",
    "print(tf.concat([r,r3], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'DEF', b'G', b'', b'HI'], dtype=object)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_encode(r3, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\n",
       "array([[   67,    97,   102,   233,     0,     0],\n",
       "       [   67,   111,   102,   102,   101,   101],\n",
       "       [   99,    97,   102,   102,   232,     0],\n",
       "       [21654, 21857,     0,     0,     0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to_tensor seems to add zeros\n",
    "r.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse tensors\n",
    "\n",
    "To efficiently use tensors that are mostly zero. `tf.sparse` for the ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices where the values are and dense_shape the shape\n",
    "s = tf.SparseTensor(indices=[[0,1],[1,0],[2,3]],\n",
    "                   values= [1.,2.,3.],\n",
    "                   dense_shape=[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [2., 0., 0., 0.],\n",
       "       [0., 0., 0., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for +: 'SparseTensor' and 'float'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s3 = s + 1.\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 30.,  40.],\n",
       "       [ 20.,  40.],\n",
       "       [210., 240.]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n",
    "tf.sparse.sparse_dense_matmul(s,s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 2]\n",
      " [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],\n",
    "                     values=[1., 2.],\n",
    "                     dense_shape=[3, 4])\n",
    "print(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices[1] = [0,1] is out of order. Many sparse ops require sorted indices.\n",
      "    Use `tf.sparse.reorder` to create a correctly ordered copy.\n",
      "\n",
      " [Op:SparseToDense]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.sparse.to_dense(s5)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 2., 1., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s6 = tf.sparse.reorder(s5)\n",
    "tf.sparse.to_dense(s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets\n",
    "\n",
    "Are represented as regular tensors. Use `tf.sets` for ops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 2,  3,  4,  5,  6,  7],\n",
       "       [ 0,  7,  9, 10,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]])\n",
    "set2 = tf.constant([[4, 5, 6], [9, 10, 0]])\n",
    "tf.sparse.to_dense(tf.sets.union(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[2, 3, 7],\n",
       "       [7, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.difference(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[5, 0],\n",
       "       [0, 9]], dtype=int32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.intersection(set1, set2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor arrays\n",
    "\n",
    "Are lists of tensors. Fixed size by default but can be dynamic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Models and Training Algorithms\n",
    "\n",
    "### Custom Loss Function\n",
    "\n",
    "Example is, training set is noisy for a regression model after cleaning it up. MSE penalizes errors too much and MAE wont penalize outliers as much but would take a while to converge on something imprecise. Example will use Huber loss.\n",
    "\n",
    "For better performance use a vectorized implementation as in the example, and if I want to use Tensorflow's graph features than only use TF ops.\n",
    "\n",
    "Also, better to return a tensor containing one loss per instance, rather than the mean loss. This is so that Keras can apply class or sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss,\n",
    "                    linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,3.5))\n",
    "z = np.linspace(-4,4,200)\n",
    "plt.plot(z,huber_fn(0,z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z**2/2,\"b:\",linewidth=1,label=r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1,-1],[0,huber_fn(0.,-1.)], \"r--\")\n",
    "plt.plot([1,1], [0,huber_fn(0.,1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color=\"k\")\n",
    "plt.gca().axvline(x=0, color=\"k\")\n",
    "plt.axis([-4,4,0,4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber Loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6235 - mae: 0.9953 - val_loss: 0.2862 - val_mae: 0.5866\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2197 - mae: 0.5177 - val_loss: 0.2382 - val_mae: 0.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1483a2cf8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "         validation_data=(X_valid_scaled,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving/Loading Models with Custom Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_custom_loss_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_custom_loss_model.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2054 - mean_absolute_error: 0.4982 - val_loss: 0.2209 - val_mean_absolute_error: 0.5050\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1999 - mean_absolute_error: 0.4900 - val_loss: 0.2127 - val_mean_absolute_error: 0.4986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14797f5c0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I want to have an adjustable threshold for error\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is saved it wont save the threshold,\n",
    "so that will have to be specified when model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2226 - mae: 0.4892 - val_loss: 0.2540 - val_mae: 0.4907\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2184 - mae: 0.4844 - val_loss: 0.2372 - val_mae: 0.4879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x146b7a080>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "                                custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2147 - mean_absolute_error: 0.4800 - val_loss: 0.2133 - val_mean_absolute_error: 0.4654\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2119 - mean_absolute_error: 0.4762 - val_loss: 0.1992 - val_mean_absolute_error: 0.4643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14a6ccf28>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since if the model needs the loss function threshold specifed, I can create a subclass of the `keras.losses.Loss` class and then implement its `get_config()` method.\n",
    "\n",
    "- Constructor method accepts the `**kwargs` and passes them to parent constructor which handles the name of the loss and the reduction algorithm to use to aggregate the individual instance loss.\n",
    "    - By default, that algorithm is 'sum_over_batch_size', which means the loss will be the sum of instance losses, weight by sample weights, and divided by batch size.\n",
    "- `call()` method takes labels and predictions, computes and instnace losses and return them.\n",
    "- `get_config()` method returns a ditctionary mapping each hyperparameter to its value. It first calls the parent class's `get_config()` then adds the new hyperparameters to this dictionary. the `{**x}` syntax unpacks the baseconfig so it goes into the returned dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self,threshold=1.0,**kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error,squared_loss,linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7892 - mae: 0.9212 - val_loss: 0.5515 - val_mae: 0.6565\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2531 - mae: 0.5172 - val_loss: 0.4497 - val_mae: 0.6041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14a988518>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_class.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when I save the model, the loss instance's `get_config()` is called by Keras and Keras saves the file as JSON in the HDF5 file. \n",
    "\n",
    "Loading the model, it calls the `from_config()` class method on HuberLoss: this class method is implemented by the base class **Loss**, which passes `**config` to the constructor. \n",
    "\n",
    "### Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equivalent to keras.activations.softplus() or\n",
    "# tf.nn.softplus\n",
    "def my_softplus(z): #return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "#equivalent to keras.initializers.glorot_normal()\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev,dtype=dtype)\n",
    "#equivalent to keras.regularizers.l1(0.01)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "#equivalent to keras.constraints.nonneg()\n",
    "def my_positive_weights(weights):\n",
    "    #return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights <0., tf.zeros_like(weights), weights)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=my_l1_regularizer,\n",
    "                          kernel_constraint=my_positive_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of these functions have hyperparameters that need to be saved along with the model, then I will need to subclass the appropriate class.\n",
    "- keras.regularizers.Regularizer\n",
    "- keras.constraints.Constraint\n",
    "- keras.initializers.Initializer\n",
    "- keras.layers.Layer\n",
    "\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.5542 - mae: 0.8962 - val_loss: 1.4154 - val_mae: 0.5607\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5943 - mae: 0.5256 - val_loss: 1.4399 - val_mae: 0.5137\n"
     ]
    }
   ],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "    \n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "\n",
    "#note that I dont have to call the value of MyL1Regularizer here\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"MyL1Regularizer\": MyL1Regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "\n",
    "Losses and metrics are different. Loss is used to *train* a model and can be uninterpretable. Metrics are used to *evaluate* a model and understood.\n",
    "\n",
    "Custom metrics are made in the same way as custom loss methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.0982 - huber_fn: 0.9192\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6052 - huber_fn: 0.2733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ad93668>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: if you use the same function as the loss and a metric, you may be surprised to see different results. This is generally just due to floating point precision errors: even though the mathematical equations are equivalent, the operations are not run in the same order, which can lead to small differences. Moreover, when using sample weights, there's more than just precision errors:\n",
    "\n",
    "- the loss since the start of the epoch is the mean of all batch losses seen so far. Each batch loss is the sum of the weighted instance losses divided by the batch size (not the sum of weights, so the batch loss is not the weighted mean of the losses).\n",
    "- the metric since the start of the epoch is equal to the sum of weighted instance losses divided by sum of all weights seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing.\n",
    "\n",
    "If you do the math, you will find that loss = metric * mean of sample weights (plus some floating point precision error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), \n",
    "              optimizer=\"nadam\", \n",
    "              metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.1175 - huber_fn: 0.2399\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1131 - huber_fn: 0.2297\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2,\n",
    "                    sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11749906837940216, 0.11906625573138947)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"huber_fn\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Metrics\n",
    "\n",
    "Also called a *stateful metric*. Most of the time the metric will be an average across an epoch, and this is generally okay. But in some cases, like precision score, the mean of the precision won't be the true precision score.\n",
    "\n",
    "Example: 5 positive predictions and 4 true positive predictions in one batch, thats 80%. 3 positive predictions and 0 true positives in second batch, 0% correct. Taking the average that is 40%, but in truth, its 50% (4 + 0) / (5 + 3). \n",
    "\n",
    "So we need to keep track of the totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self,threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) #handles base args(e.g. dtype)\n",
    "        self.threshold = threshold\n",
    "        # creating variables to track sum of all huber losses\n",
    "        self.total = self.add_weight(\"total\", initializer='zeros')\n",
    "        # and number of instances seen so fat\n",
    "        self.count = self.add_weight(\"count\", initializer='zeros')\n",
    "    def huber_fn(self,y_true,y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true),tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=14.0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = HuberMetric(2.)\n",
    "\n",
    "# total = 2 * |10 - 2| - 2²/2 = 14\n",
    "# count = 1\n",
    "# result = 14 / 1 = 14\n",
    "m(tf.constant([[2.]]), tf.constant([[10.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could have also implemented the metric like so\n",
    "# handles shape better and supports sample weights\n",
    "\n",
    "class HuberMetric(keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWarning: In TF 2.2, tf.keras adds an \\nextra first metric in model.metrics at position 0 \\n(see TF issue #38150). This forces us to \\nuse model.metrics[-1] rather than model.metrics[0]\\nto access the HuberMetric.\\n'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Warning: In TF 2.2, tf.keras adds an \n",
    "extra first metric in model.metrics at position 0 \n",
    "(see TF issue #38150). This forces us to \n",
    "use model.metrics[-1] rather than model.metrics[0]\n",
    "to access the HuberMetric.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "\n",
    "For when there is no default implementation of an architecure that I want, or a repetitive patterned architecure.\n",
    "\n",
    "If I want to create a layer without any weights, `keras.layers.Lambda`. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponential_layer([-1.,0.,1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be used with any of the API's (Sequential,...)\n",
    "\n",
    "Adding an exponential layer at the output of a regression model can be useful if the values to predict are positive and with very different scales (e.g., 0.001, 10., 10000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - ETA: 0s - loss: n - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "162/162 [==============================] - 0s 1ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "    exponential_layer\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a layer with weights or a *stateful* layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    #takes all hyperparameters as args\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        #calls the parent constructor, passing it the kwargs\n",
    "        #taking care of the standard args like input_shape and name\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        #so that it will accept activation funcstions or \n",
    "        #just regular strings like relu or selu\n",
    "        self.activation = keras.activations.get(activation)\n",
    "# build functions role is to create layer variables by \n",
    "# calling the add_weight for each weight.\n",
    "# its called the first time the new layers is used\n",
    "    def build(self, batch_input_shape):\n",
    "        #kernel is the connection weights matrix\n",
    "        #reason batch_input_shape[-1] is b/c we need to know the size of the \n",
    "        #last dimension of inputs\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        #must call this at the end, calls parent constructor\n",
    "        #to set self.built=True and the layer is done\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        #will compute the output of the layer\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        #for this e.g., same shape as inputs, just replacing last dimension\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        #so that we can save the configs\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}\n",
    "        #Saving the activation functios full config as keras.activations.serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can generally omit the compute_output_shape(), tf.keras will infer the output shape, except when layer is dynamic. For other Keras implementations, it is required or it assumes that the output shape is th same as input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.2563 - val_loss: 0.9472\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6485 - val_loss: 0.6219\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.5474\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "model.save(\"my_model_with_a_custom_layer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_layer.h5\",\n",
    "                                custom_objects={\"MyDense\": MyDense})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a layer with multiple inputs, the `call()` method needs to be a tuple containing the inputs, the `compute_output_shape()` should also be a tuple with each input's batch shape.\n",
    "\n",
    "For a layer with multiple outputs, `call()` returns the list of outputs, and `compute_output_shape()` returns the list of batch output shapes (one per output).\n",
    "\n",
    "E.g. Takes two inputs and return 3 outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1,X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1,b1,b1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer can't be used by the Sequential API, since that is only one input and one output.\n",
    "\n",
    "If the layer needs to have different behaviour during training vs testing, then will have to add a `training` argument to the `call()` method. \n",
    "\n",
    "For e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self,stddev,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom models\n",
    "\n",
    "Example below doesn't make sense, just for example purposes. Shows you can do loops and skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons,\n",
    "                                         activation=\"elu\",\n",
    "                                         kernel_initializer=\"he_normal\") \n",
    "                       for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                         kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2,30)\n",
    "        self.block2 = ResidualBlock(2,30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        z = self.block2(Z)\n",
    "        return self.out(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the layers in the constructor and use them in the call() method. To save the model need the get_config() method in both custom layer and model to use load_model() or save_weights() and load_weights().\n",
    "\n",
    "The `Model` class is actually a subclass of the `Layer` class but Model has a few more functions and its better to be able to distinguish between the parts for readability. \n",
    "\n",
    "## Losses and Metrics based on Model Internals\n",
    "\n",
    "The previous custom losses and metrics were all based on the labels and the predictions. Sometimes, we want to define losses based on other parts of the model, such as weights or activations of hidden layers. Useful for regularization purposes or to monitor internal aspects of the model.\n",
    "\n",
    "Defining a custom loss based on model internals, compute it based on what yo want to measure, then pass result to add_loss() method. \n",
    "\n",
    "E.g. below: Custom regression MLP model, stack of 5 hidden layers plus output. Also, auxiiliary output on top of the upper hidden layer. The loss associated with this aux output will be call the *reconstruction loss* (the mean squared difference between reconstruction and the inputs). Adding this reconstruction loss to the main loss we encourage the model to keep more information, even not useful information, which helps with generalization. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        # TODO: check https://github.com/tensorflow/tensorflow/issues/26260\n",
    "        #self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        #if training:\n",
    "        #    result = self.reconstruction_mean(recon_loss)\n",
    "        #    self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constructor creates the DNN with 5 dense layers and 1 output layer\n",
    "\n",
    "- `build()` creates an extra dense layer which will be used to reconstruct the inputs of the model. Has to be created here bc # of units must equal # of inputs, and number is unknown before build() is called.\n",
    " \n",
    "- call() processes inputs through all 5 hiddfen layers, then passes them through the reconstruction layer, which produces the reconstruction.\n",
    "\n",
    "- call() computes the reconstruction loss and addes it to the model's list of losses using `add_loss()`. The recon_loss is scaled down so as not to overpower the main loss. \n",
    "\n",
    "- call() method passes the output of hidden layers to output layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:230 __call__\n        reg_loss = math_ops.add_n(regularization_losses)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3512 add_n\n        return gen_math_ops.add_n(inputs, name=name)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:432 add_n\n        \"AddN\", inputs=inputs, name=name)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:589 _create_op_internal\n        inp = self.capture(inp)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:639 capture\n        % (tensor, tensor.graph, self))\n\n    InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=5554482312); accessed from: FuncGraph(name=train_function, id=5554432208).\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-02c0c12e003a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReconstructingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:230 __call__\n        reg_loss = math_ops.add_n(regularization_losses)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3512 add_n\n        return gen_math_ops.add_n(inputs, name=name)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:432 add_n\n        \"AddN\", inputs=inputs, name=name)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:589 _create_op_internal\n        inp = self.capture(inp)\n    /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:639 capture\n        % (tensor, tensor.graph, self))\n\n    InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=5554482312); accessed from: FuncGraph(name=train_function, id=5554432208).\n    \n"
     ]
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
